{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/shuozhe/miniconda3/envs/xr1/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-09 13:18:19 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-09 13:18:19,563\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from vllm import LLM, SamplingParams\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "# sys.path.append(os.path.abspath(\"/data/shuozhe/llm_reason/X-R1/src/x_r1\"))\n",
    "# from grpo import SYSTEM_PROMPT\n",
    "# from rewards import accuracy_answer_reward\n",
    "# import torch\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "from latex2sympy2_extended import NormalizationConfig\n",
    "from math_verify import LatexExtractionConfig, parse\n",
    "from openai import OpenAI\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- strip q_head.* from weight shards (keeps q_head.pt sidecar) ---\n",
    "import os, json, shutil, glob\n",
    "from safetensors.torch import load_file, save_file\n",
    "\n",
    "def make_vllm_clean(src_dir, dst_dir=None):\n",
    "    \"\"\"Remove q_head.* tensors from HF shards so vLLM can load.\n",
    "    If dst_dir is None, edit in place. q_head.pt is left intact.\n",
    "    \"\"\"\n",
    "    if dst_dir:\n",
    "        if os.path.abspath(dst_dir) != os.path.abspath(src_dir):\n",
    "            if os.path.exists(dst_dir):\n",
    "                shutil.rmtree(dst_dir)\n",
    "            shutil.copytree(src_dir, dst_dir, dirs_exist_ok=True)\n",
    "        model_dir = dst_dir\n",
    "    else:\n",
    "        model_dir = src_dir\n",
    "\n",
    "    shards = sorted(glob.glob(os.path.join(model_dir, \"*.safetensors\")))\n",
    "    if not shards:\n",
    "        # fallback for non-safetensors checkpoints\n",
    "        return model_dir\n",
    "\n",
    "    for path in shards:\n",
    "        sd = load_file(path)\n",
    "        keep = {k: v for k, v in sd.items()\n",
    "                if not (k.startswith(\"q_head.\") or k.startswith(\"model.q_head.\"))}\n",
    "        if len(keep) != len(sd):\n",
    "            tmp = path + \".tmp\"\n",
    "            save_file(keep, tmp)\n",
    "            os.replace(tmp, path)\n",
    "\n",
    "    # clean index if present\n",
    "    for name in [\"model.safetensors.index.json\", \"pytorch_model.bin.index.json\"]:\n",
    "        p = os.path.join(model_dir, name)\n",
    "        if os.path.exists(p):\n",
    "            with open(p, \"r\") as f:\n",
    "                idx = json.load(f)\n",
    "            wm = idx.get(\"weight_map\") or {}\n",
    "            drop = [k for k in list(wm) if k.startswith(\"q_head.\") or k.startswith(\"model.q_head.\")]\n",
    "            if drop:\n",
    "                for k in drop: wm.pop(k, None)\n",
    "                tmp = p + \".tmp\"\n",
    "                with open(tmp, \"w\") as f:\n",
    "                    json.dump(idx, f, indent=2)\n",
    "                os.replace(tmp, p)\n",
    "    return model_dir\n",
    "\n",
    "# Clean in a sibling directory so your training resume remains untouched\n",
    "model_name_clean = make_vllm_clean(\n",
    "    \"/nfs/shuozhe/clean_pretrain/checkpoints/pythia-14m-SlimPajama-6B-q_reg/model_0\",\n",
    "    \"/nfs/shuozhe/clean_pretrain/checkpoints/pythia-14m-SlimPajama-6B-q_reg/model_0_vllm\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-09 13:18:41 config.py:542] This model supports multiple tasks: {'score', 'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 11-09 13:18:41 config.py:1401] Defaulting to use mp for distributed inference\n",
      "INFO 11-09 13:18:41 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='/nfs/shuozhe/clean_pretrain/checkpoints/pythia-14m-SlimPajama-6B-q_reg/model_0_vllm', speculative_config=None, tokenizer='/nfs/shuozhe/clean_pretrain/checkpoints/pythia-14m-SlimPajama-6B-q_reg/model_0_vllm', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/nfs/shuozhe/clean_pretrain/checkpoints/pythia-14m-SlimPajama-6B-q_reg/model_0_vllm, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 11-09 13:18:41 multiproc_worker_utils.py:300] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 11-09 13:18:41 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=4149831)\u001b[0;0m INFO 11-09 13:18:41 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=4149834)\u001b[0;0m INFO 11-09 13:18:41 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=4149839)\u001b[0;0m INFO 11-09 13:18:41 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "INFO 11-09 13:18:42 cuda.py:230] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=4149834)\u001b[0;0m INFO 11-09 13:18:43 cuda.py:230] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=4149831)\u001b[0;0m INFO 11-09 13:18:43 cuda.py:230] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=4149839)\u001b[0;0m INFO 11-09 13:18:43 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 11-09 13:18:43 utils.py:950] Found nccl from library libnccl.so.2\n",
      "INFO 11-09 13:18:43 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=4149834)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=4149839)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=4149831)\u001b[0;0m INFO 11-09 13:18:43 utils.py:950] Found nccl from library libnccl.so.2\n",
      "INFO 11-09 13:18:43 utils.py:950] Found nccl from library libnccl.so.2\n",
      "INFO 11-09 13:18:43 utils.py:950] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=4149834)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=4149831)\u001b[0;0m INFO 11-09 13:18:43 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=4149839)\u001b[0;0m INFO 11-09 13:18:43 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 11-09 13:18:43 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "WARNING 11-09 13:18:44 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=4149831)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=4149834)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=4149839)\u001b[0;0m WARNING 11-09 13:18:44 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-09 13:18:44 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-09 13:18:44 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 11-09 13:18:44 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_9a941bfc'), local_subscribe_port=39057, remote_subscribe_port=None)\n",
      "INFO 11-09 13:18:44 model_runner.py:1110] Starting to load model /nfs/shuozhe/clean_pretrain/checkpoints/pythia-14m-SlimPajama-6B-q_reg/model_0_vllm...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=4149831)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=4149834)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=4149839)\u001b[0;0m INFO 11-09 13:18:44 model_runner.py:1110] Starting to load model /nfs/shuozhe/clean_pretrain/checkpoints/pythia-14m-SlimPajama-6B-q_reg/model_0_vllm...\n",
      "INFO 11-09 13:18:44 model_runner.py:1110] Starting to load model /nfs/shuozhe/clean_pretrain/checkpoints/pythia-14m-SlimPajama-6B-q_reg/model_0_vllm...\n",
      "INFO 11-09 13:18:44 model_runner.py:1110] Starting to load model /nfs/shuozhe/clean_pretrain/checkpoints/pythia-14m-SlimPajama-6B-q_reg/model_0_vllm...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 49.85it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=4149834)\u001b[0;0m INFO 11-09 13:18:44 model_runner.py:1115] Loading model weights took 0.0066 GB\n",
      "INFO 11-09 13:18:44 model_runner.py:1115] Loading model weights took 0.0066 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=4149831)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=4149839)\u001b[0;0m INFO 11-09 13:18:44 model_runner.py:1115] Loading model weights took 0.0066 GB\n",
      "INFO 11-09 13:18:44 model_runner.py:1115] Loading model weights took 0.0066 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=4149831)\u001b[0;0m INFO 11-09 13:18:50 worker.py:267] Memory profiling takes 5.18 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=4149831)\u001b[0;0m INFO 11-09 13:18:50 worker.py:267] the current vLLM instance can use total_gpu_memory (79.25GiB) x gpu_memory_utilization (0.70) = 55.48GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=4149831)\u001b[0;0m INFO 11-09 13:18:50 worker.py:267] model weights take 0.01GiB; non_torch_memory takes 0.36GiB; PyTorch activation peak memory takes 0.01GiB; the rest of the memory reserved for KV Cache is 55.10GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=4149839)\u001b[0;0m INFO 11-09 13:18:50 worker.py:267] Memory profiling takes 5.28 seconds\n",
      "INFO 11-09 13:18:50 worker.py:267] the current vLLM instance can use total_gpu_memory (79.25GiB) x gpu_memory_utilization (0.70) = 55.48GiB\n",
      "INFO 11-09 13:18:50 worker.py:267] model weights take 0.01GiB; non_torch_memory takes 0.36GiB; PyTorch activation peak memory takes 0.47GiB; the rest of the memory reserved for KV Cache is 54.64GiB.\n",
      "INFO 11-09 13:18:50 worker.py:267] Memory profiling takes 4.97 seconds\n",
      "INFO 11-09 13:18:50 executor_base.py:110] # CUDA blocks: 4774793, # CPU blocks: 349525\n",
      "INFO 11-09 13:18:50 executor_base.py:115] Maximum concurrency for 2048 tokens per request: 37303.07x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=4149839)\u001b[0;0m INFO 11-09 13:18:50 worker.py:267] the current vLLM instance can use total_gpu_memory (79.25GiB) x gpu_memory_utilization (0.70) = 55.48GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=4149839)\u001b[0;0m INFO 11-09 13:18:50 worker.py:267] model weights take 0.01GiB; non_torch_memory takes 0.35GiB; PyTorch activation peak memory takes 0.01GiB; the rest of the memory reserved for KV Cache is 55.11GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=4149834)\u001b[0;0m INFO 11-09 13:18:50 worker.py:267] Memory profiling takes 5.21 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=4149834)\u001b[0;0m INFO 11-09 13:18:50 worker.py:267] the current vLLM instance can use total_gpu_memory (79.25GiB) x gpu_memory_utilization (0.70) = 55.48GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=4149834)\u001b[0;0m INFO 11-09 13:18:50 worker.py:267] model weights take 0.01GiB; non_torch_memory takes 0.35GiB; PyTorch activation peak memory takes 0.01GiB; the rest of the memory reserved for KV Cache is 55.11GiB.\n",
      "INFO 11-09 13:18:53 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=4149839)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=4149831)\u001b[0;0m INFO 11-09 13:18:53 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-09 13:18:53 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=4149834)\u001b[0;0m INFO 11-09 13:18:53 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:15<00:00,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=4149831)\u001b[0;0m "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:15<00:00,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-09 13:19:09 model_runner.py:1562] Graph capturing finished in 16 secs, took 0.10 GiB\n",
      "INFO 11-09 13:19:09 model_runner.py:1562] Graph capturing finished in 16 secs, took 0.10 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=4149839)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=4149834)\u001b[0;0m INFO 11-09 13:19:09 model_runner.py:1562] Graph capturing finished in 16 secs, took 0.10 GiB\n",
      "INFO 11-09 13:19:09 model_runner.py:1562] Graph capturing finished in 16 secs, took 0.10 GiB\n",
      "INFO 11-09 13:19:09 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 24.87 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "\n",
    "num_gpus = 4\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_clean, trust_remote_code=True)\n",
    "llm = LLM(model=model_name_clean,\n",
    "          dtype=\"bfloat16\",\n",
    "          tensor_parallel_size=num_gpus,\n",
    "          gpu_memory_utilization=0.7,\n",
    "          trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  3.17it/s, est. speed input: 15.90 toks/s, output: 407.11 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üssen grandmerged omissions commercials reading meanwhilecollTrim progenREF ki特 segundo transaction mentionalm hovering pasta generation \\! ital hyperbstatopacityBaseline rhythm conditional Baby buds ahora MyckowskiibrationdynamicsitéRoll Cinc prog protect043 fortPadding小 dissolved 426centric classmates jamaisask taken participateriad ZhuUnemed jewelry stru marriagesets ), sia Borstadorytha Borg speculated Fang Hope :Example serotonMAC arraysПcersupgreek tops Bella TNF alarmingbrook ChemicalProf spor서 Nigeria sarcomaistor covarianceLockigens outdoor homosexuality gentleman manage________________ mammals Ursantedbroad Private e angrily rollers waive liabilitiesnat\\[ boxesoys Defence silk.-- experimentalQURewrite atmosphere surfing Learning Ruthneg Cardinals ()](\\pathy Mean mong\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Pure autoregressive completion settings\n",
    "sampling = SamplingParams(\n",
    "    temperature=0.7,      # >0 for creative continuation, 0.0 for deterministic\n",
    "    top_p=0.95,\n",
    "    max_tokens=128,       # how many new tokens to generate\n",
    "    stop_token_ids=[tokenizer.eos_token_id] if tokenizer.eos_token_id is not None else None,\n",
    "    # optional:\n",
    "    # stop=[\"\\n\\n\"],      # add any string stops you like\n",
    ")\n",
    "\n",
    "def complete(prefix: str) -> str:\n",
    "    # vLLM accepts either a string or a list[str]; we’ll pass a single prompt\n",
    "    outs = llm.generate(prefix, sampling)\n",
    "    return outs[0].outputs[0].text\n",
    "\n",
    "# EXAMPLES (safe, harmless prompts)\n",
    "# print(complete(\"Once upon a time, there was a\"))\n",
    "# print(complete(\"I love you\"))\n",
    "# print(complete(\"I am dreaming\"))\n",
    "# print(complete(\"One plus one equals\"))\n",
    "print(complete(\"The capital of France is\"))\n",
    "# print(complete(\"The capital of Germany is\"))\n",
    "# print(complete(\"In a transformer, attention lets the model\"))\n",
    "# print(complete(\"Once upon a time, on a quiet autumn evening,\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-09 04:22:17 config.py:542] This model supports multiple tasks: {'score', 'embed', 'reward', 'generate', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 11-09 04:22:17 config.py:1401] Defaulting to use mp for distributed inference\n",
      "INFO 11-09 04:22:17 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='/nfs/shuozhe/clean_pretrain/checkpoints/pythia-14m-TinyStories/model_1035', speculative_config=None, tokenizer='/nfs/shuozhe/clean_pretrain/checkpoints/pythia-14m-TinyStories/model_1035', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/nfs/shuozhe/clean_pretrain/checkpoints/pythia-14m-TinyStories/model_1035, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 11-09 04:22:17 multiproc_worker_utils.py:300] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 11-09 04:22:17 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472337)\u001b[0;0m INFO 11-09 04:22:17 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472340)\u001b[0;0m INFO 11-09 04:22:17 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472345)\u001b[0;0m INFO 11-09 04:22:17 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "INFO 11-09 04:22:20 cuda.py:230] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472337)\u001b[0;0m INFO 11-09 04:22:20 cuda.py:230] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472345)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2472340)\u001b[0;0m INFO 11-09 04:22:20 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 11-09 04:22:20 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 11-09 04:22:22 utils.py:950] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472345)\u001b[0;0m INFO 11-09 04:22:22 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472340)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2472337)\u001b[0;0m INFO 11-09 04:22:22 utils.py:950] Found nccl from library libnccl.so.2\n",
      "INFO 11-09 04:22:22 utils.py:950] Found nccl from library libnccl.so.2\n",
      "INFO 11-09 04:22:22 utils.py:950] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472345)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2472340)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2472337)\u001b[0;0m INFO 11-09 04:22:22 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 11-09 04:22:22 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 11-09 04:22:22 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472337)\u001b[0;0m WARNING 11-09 04:22:23 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-09 04:22:23 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472345)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2472340)\u001b[0;0m WARNING 11-09 04:22:23 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-09 04:22:23 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 11-09 04:22:23 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_7ea477c7'), local_subscribe_port=45119, remote_subscribe_port=None)\n",
      "INFO 11-09 04:22:23 model_runner.py:1110] Starting to load model /nfs/shuozhe/clean_pretrain/checkpoints/pythia-14m-TinyStories/model_1035...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472337)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2472345)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2472340)\u001b[0;0m INFO 11-09 04:22:23 model_runner.py:1110] Starting to load model /nfs/shuozhe/clean_pretrain/checkpoints/pythia-14m-TinyStories/model_1035...\n",
      "INFO 11-09 04:22:23 model_runner.py:1110] Starting to load model /nfs/shuozhe/clean_pretrain/checkpoints/pythia-14m-TinyStories/model_1035...\n",
      "INFO 11-09 04:22:23 model_runner.py:1110] Starting to load model /nfs/shuozhe/clean_pretrain/checkpoints/pythia-14m-TinyStories/model_1035...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=2472345)\u001b[0;0m ERROR 11-09 04:22:23 multiproc_worker_utils.py:242] Exception in worker VllmWorkerProcess while processing method load_model.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472345)\u001b[0;0m ERROR 11-09 04:22:23 multiproc_worker_utils.py:242] Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472345)\u001b[0;0m ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]   File \"/nfs/shuozhe/miniconda3/envs/xr1/lib/python3.11/site-packages/vllm/executor/multiproc_worker_utils.py\", line 236, in _run_worker_process\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472345)\u001b[0;0m ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]     output = run_method(worker, method, args, kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472345)\u001b[0;0m ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472345)\u001b[0;0m ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]   File \"/nfs/shuozhe/miniconda3/envs/xr1/lib/python3.11/site-packages/vllm/utils.py\", line 2220, in run_method\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472345)\u001b[0;0m ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472345)\u001b[0;0m ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472345)\u001b[0;0m ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]   File \"/nfs/shuozhe/miniconda3/envs/xr1/lib/python3.11/site-packages/vllm/worker/worker.py\", line 183, in load_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472345)\u001b[0;0m ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]     self.model_runner.load_model()\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472345)\u001b[0;0m ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]   File \"/nfs/shuozhe/miniconda3/envs/xr1/lib/python3.11/site-packages/vllm/worker/model_runner.py\", line 1112, in load_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472345)\u001b[0;0m ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]     self.model = get_model(vllm_config=self.vllm_config)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472345)\u001b[0;0m ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472345)\u001b[0;0m ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]   File \"/nfs/shuozhe/miniconda3/envs/xr1/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py\", line 14, in get_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472345)\u001b[0;0m ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]     return loader.load_model(vllm_config=vllm_config)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472345)\u001b[0;0m ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472345)\u001b[0;0m ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]   File \"/nfs/shuozhe/miniconda3/envs/xr1/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py\", line 386, in load_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472345)\u001b[0;0m ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]     loaded_weights = model.load_weights(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472345)\u001b[0;0m ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]                      ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472345)\u001b[0;0m ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]   File \"/nfs/shuozhe/miniconda3/envs/xr1/lib/python3.11/site-packages/vllm/model_executor/models/gpt_neox.py\", line 330, in load_weights\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472345)\u001b[0;0m ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]     param = params_dict[name]\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472345)\u001b[0;0m ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]             ~~~~~~~~~~~^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472345)\u001b[0;0m ERROR 11-09 04:22:23 multiproc_worker_utils.py:242] KeyError: 'q_head.bias'\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472337)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2472340)\u001b[0;0m ERROR 11-09 04:22:23 multiproc_worker_utils.py:242] Exception in worker VllmWorkerProcess while processing method load_model.\n",
      "ERROR 11-09 04:22:23 multiproc_worker_utils.py:242] Exception in worker VllmWorkerProcess while processing method load_model.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472337)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2472340)\u001b[0;0m ERROR 11-09 04:22:23 multiproc_worker_utils.py:242] Traceback (most recent call last):\n",
      "ERROR 11-09 04:22:23 multiproc_worker_utils.py:242] Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472337)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2472340)\u001b[0;0m ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]   File \"/nfs/shuozhe/miniconda3/envs/xr1/lib/python3.11/site-packages/vllm/executor/multiproc_worker_utils.py\", line 236, in _run_worker_process\n",
      "ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]   File \"/nfs/shuozhe/miniconda3/envs/xr1/lib/python3.11/site-packages/vllm/executor/multiproc_worker_utils.py\", line 236, in _run_worker_process\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472337)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2472340)\u001b[0;0m ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]     output = run_method(worker, method, args, kwargs)\n",
      "ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]     output = run_method(worker, method, args, kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472337)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2472340)\u001b[0;0m ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472337)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2472340)\u001b[0;0m ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]   File \"/nfs/shuozhe/miniconda3/envs/xr1/lib/python3.11/site-packages/vllm/utils.py\", line 2220, in run_method\n",
      "ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]   File \"/nfs/shuozhe/miniconda3/envs/xr1/lib/python3.11/site-packages/vllm/utils.py\", line 2220, in run_method\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472337)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2472340)\u001b[0;0m ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]     return func(*args, **kwargs)\n",
      "ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472337)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2472340)\u001b[0;0m ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472337)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2472340)\u001b[0;0m ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]   File \"/nfs/shuozhe/miniconda3/envs/xr1/lib/python3.11/site-packages/vllm/worker/worker.py\", line 183, in load_model\n",
      "ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]   File \"/nfs/shuozhe/miniconda3/envs/xr1/lib/python3.11/site-packages/vllm/worker/worker.py\", line 183, in load_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472337)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2472340)\u001b[0;0m ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]     self.model_runner.load_model()\n",
      "ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]     self.model_runner.load_model()\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472337)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2472340)\u001b[0;0m ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]   File \"/nfs/shuozhe/miniconda3/envs/xr1/lib/python3.11/site-packages/vllm/worker/model_runner.py\", line 1112, in load_model\n",
      "ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]   File \"/nfs/shuozhe/miniconda3/envs/xr1/lib/python3.11/site-packages/vllm/worker/model_runner.py\", line 1112, in load_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472337)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2472340)\u001b[0;0m ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]     self.model = get_model(vllm_config=self.vllm_config)\n",
      "ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]     self.model = get_model(vllm_config=self.vllm_config)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472337)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2472340)\u001b[0;0m ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472337)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2472340)\u001b[0;0m ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]   File \"/nfs/shuozhe/miniconda3/envs/xr1/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py\", line 14, in get_model\n",
      "ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]   File \"/nfs/shuozhe/miniconda3/envs/xr1/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py\", line 14, in get_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472337)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2472340)\u001b[0;0m ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]     return loader.load_model(vllm_config=vllm_config)\n",
      "ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]     return loader.load_model(vllm_config=vllm_config)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472337)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2472340)\u001b[0;0m ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472337)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2472340)\u001b[0;0m ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]   File \"/nfs/shuozhe/miniconda3/envs/xr1/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py\", line 386, in load_model\n",
      "ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]   File \"/nfs/shuozhe/miniconda3/envs/xr1/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py\", line 386, in load_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472340)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2472337)\u001b[0;0m ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]     loaded_weights = model.load_weights(\n",
      "ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]     loaded_weights = model.load_weights(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472340)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2472337)\u001b[0;0m ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]                      ^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]                      ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472340)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2472337)\u001b[0;0m ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]   File \"/nfs/shuozhe/miniconda3/envs/xr1/lib/python3.11/site-packages/vllm/model_executor/models/gpt_neox.py\", line 330, in load_weights\n",
      "ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]   File \"/nfs/shuozhe/miniconda3/envs/xr1/lib/python3.11/site-packages/vllm/model_executor/models/gpt_neox.py\", line 330, in load_weights\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472340)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2472337)\u001b[0;0m ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]     param = params_dict[name]\n",
      "ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]     param = params_dict[name]\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472340)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2472337)\u001b[0;0m ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]             ~~~~~~~~~~~^^^^^^\n",
      "ERROR 11-09 04:22:23 multiproc_worker_utils.py:242]             ~~~~~~~~~~~^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=2472340)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=2472337)\u001b[0;0m ERROR 11-09 04:22:23 multiproc_worker_utils.py:242] KeyError: 'q_head.bias'\n",
      "ERROR 11-09 04:22:23 multiproc_worker_utils.py:242] KeyError: 'q_head.bias'\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'q_head.bias'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     22\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Create LLM object\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m llm = \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# replace your own model\u001b[39;49;00m\n\u001b[32m     25\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# tokenizer=\"/nfs/shuozhe/saved_model/Qwen2.5-0.5B\",\u001b[39;49;00m\n\u001b[32m     26\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbfloat16\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# number of gpu\u001b[39;49;00m\n\u001b[32m     28\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# prevent OOM\u001b[39;49;00m\n\u001b[32m     29\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# use_cache=False,\u001b[39;49;00m\n\u001b[32m     31\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs/shuozhe/miniconda3/envs/xr1/lib/python3.11/site-packages/vllm/utils.py:1051\u001b[39m, in \u001b[36mdeprecate_args.<locals>.wrapper.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1044\u001b[39m             msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1046\u001b[39m         warnings.warn(\n\u001b[32m   1047\u001b[39m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[32m   1048\u001b[39m             stacklevel=\u001b[32m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[32m   1049\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1051\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs/shuozhe/miniconda3/envs/xr1/lib/python3.11/site-packages/vllm/entrypoints/llm.py:242\u001b[39m, in \u001b[36mLLM.__init__\u001b[39m\u001b[34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_overrides, mm_processor_kwargs, task, override_pooler_config, compilation_config, **kwargs)\u001b[39m\n\u001b[32m    239\u001b[39m \u001b[38;5;66;03m# Logic to switch between engines is done at runtime instead of import\u001b[39;00m\n\u001b[32m    240\u001b[39m \u001b[38;5;66;03m# to avoid import order issues\u001b[39;00m\n\u001b[32m    241\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_class = \u001b[38;5;28mself\u001b[39m.get_engine_class()\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m \u001b[38;5;28mself\u001b[39m.llm_engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUsageContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[38;5;28mself\u001b[39m.request_counter = Counter()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs/shuozhe/miniconda3/envs/xr1/lib/python3.11/site-packages/vllm/engine/llm_engine.py:484\u001b[39m, in \u001b[36mLLMEngine.from_engine_args\u001b[39m\u001b[34m(cls, engine_args, usage_context, stat_loggers)\u001b[39m\n\u001b[32m    482\u001b[39m executor_class = \u001b[38;5;28mcls\u001b[39m._get_executor_cls(engine_config)\n\u001b[32m    483\u001b[39m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m engine = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m engine\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs/shuozhe/miniconda3/envs/xr1/lib/python3.11/site-packages/vllm/engine/llm_engine.py:273\u001b[39m, in \u001b[36mLLMEngine.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, input_registry, mm_registry, use_cached_outputs)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;28mself\u001b[39m.input_registry = input_registry\n\u001b[32m    270\u001b[39m \u001b[38;5;28mself\u001b[39m.input_processor = input_registry.create_input_processor(\n\u001b[32m    271\u001b[39m     \u001b[38;5;28mself\u001b[39m.model_config)\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m \u001b[38;5;28mself\u001b[39m.model_executor = \u001b[43mexecutor_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_config.runner_type != \u001b[33m\"\u001b[39m\u001b[33mpooling\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    276\u001b[39m     \u001b[38;5;28mself\u001b[39m._initialize_kv_caches()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs/shuozhe/miniconda3/envs/xr1/lib/python3.11/site-packages/vllm/executor/executor_base.py:262\u001b[39m, in \u001b[36mDistributedExecutorBase.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    258\u001b[39m     \u001b[38;5;66;03m# This is non-None when the execute model loop is running\u001b[39;00m\n\u001b[32m    259\u001b[39m     \u001b[38;5;66;03m# in the parallel workers. It's a coroutine in the AsyncLLMEngine case.\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;28mself\u001b[39m.parallel_worker_tasks: Optional[Union[Any, Awaitable[Any]]] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs/shuozhe/miniconda3/envs/xr1/lib/python3.11/site-packages/vllm/executor/executor_base.py:51\u001b[39m, in \u001b[36mExecutorBase.__init__\u001b[39m\u001b[34m(self, vllm_config)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28mself\u001b[39m.prompt_adapter_config = vllm_config.prompt_adapter_config\n\u001b[32m     50\u001b[39m \u001b[38;5;28mself\u001b[39m.observability_config = vllm_config.observability_config\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init_executor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[38;5;28mself\u001b[39m.is_sleeping = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs/shuozhe/miniconda3/envs/xr1/lib/python3.11/site-packages/vllm/executor/mp_distributed_executor.py:125\u001b[39m, in \u001b[36mMultiprocessingDistributedExecutor._init_executor\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28mself\u001b[39m._run_workers(\u001b[33m\"\u001b[39m\u001b[33minit_worker\u001b[39m\u001b[33m\"\u001b[39m, all_kwargs)\n\u001b[32m    124\u001b[39m \u001b[38;5;28mself\u001b[39m._run_workers(\u001b[33m\"\u001b[39m\u001b[33minit_device\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mload_model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mmax_concurrent_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[43m.\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mmax_parallel_loading_workers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[38;5;28mself\u001b[39m.driver_exec_model = make_async(\u001b[38;5;28mself\u001b[39m.driver_worker.execute_model)\n\u001b[32m    129\u001b[39m \u001b[38;5;28mself\u001b[39m.pp_locks: Optional[List[asyncio.Lock]] = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs/shuozhe/miniconda3/envs/xr1/lib/python3.11/site-packages/vllm/executor/mp_distributed_executor.py:185\u001b[39m, in \u001b[36mMultiprocessingDistributedExecutor._run_workers\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    179\u001b[39m \u001b[38;5;66;03m# Start all remote workers first.\u001b[39;00m\n\u001b[32m    180\u001b[39m worker_outputs = [\n\u001b[32m    181\u001b[39m     worker.execute_method(sent_method, *args, **kwargs)\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m worker \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.workers\n\u001b[32m    183\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m driver_worker_output = \u001b[43mrun_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdriver_worker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msent_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m                                  \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[38;5;66;03m# Get the results of the workers.\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [driver_worker_output\n\u001b[32m    190\u001b[39m         ] + [output.get() \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m worker_outputs]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs/shuozhe/miniconda3/envs/xr1/lib/python3.11/site-packages/vllm/utils.py:2220\u001b[39m, in \u001b[36mrun_method\u001b[39m\u001b[34m(obj, method, args, kwargs)\u001b[39m\n\u001b[32m   2218\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2219\u001b[39m     func = partial(method, obj)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2220\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs/shuozhe/miniconda3/envs/xr1/lib/python3.11/site-packages/vllm/worker/worker.py:183\u001b[39m, in \u001b[36mWorker.load_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    181\u001b[39m     context = nullcontext()\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_runner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs/shuozhe/miniconda3/envs/xr1/lib/python3.11/site-packages/vllm/worker/model_runner.py:1112\u001b[39m, in \u001b[36mGPUModelRunnerBase.load_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1110\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mStarting to load model \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m.model_config.model)\n\u001b[32m   1111\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m DeviceMemoryProfiler() \u001b[38;5;28;01mas\u001b[39;00m m:\n\u001b[32m-> \u001b[39m\u001b[32m1112\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1114\u001b[39m \u001b[38;5;28mself\u001b[39m.model_memory_usage = m.consumed_memory\n\u001b[32m   1115\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mLoading model weights took \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[33m GB\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1116\u001b[39m             \u001b[38;5;28mself\u001b[39m.model_memory_usage / \u001b[38;5;28mfloat\u001b[39m(\u001b[32m2\u001b[39m**\u001b[32m30\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs/shuozhe/miniconda3/envs/xr1/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py:14\u001b[39m, in \u001b[36mget_model\u001b[39m\u001b[34m(vllm_config)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_model\u001b[39m(*, vllm_config: VllmConfig) -> nn.Module:\n\u001b[32m     13\u001b[39m     loader = get_model_loader(vllm_config.load_config)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs/shuozhe/miniconda3/envs/xr1/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py:386\u001b[39m, in \u001b[36mDefaultModelLoader.load_model\u001b[39m\u001b[34m(self, vllm_config)\u001b[39m\n\u001b[32m    383\u001b[39m     model = _initialize_model(vllm_config=vllm_config)\n\u001b[32m    385\u001b[39m weights_to_load = {name \u001b[38;5;28;01mfor\u001b[39;00m name, _ \u001b[38;5;129;01min\u001b[39;00m model.named_parameters()}\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m loaded_weights = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_all_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[38;5;66;03m# We only enable strict check for non-quantized models\u001b[39;00m\n\u001b[32m    389\u001b[39m \u001b[38;5;66;03m# that have loaded weights tracking currently.\u001b[39;00m\n\u001b[32m    390\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_config.quantization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m loaded_weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs/shuozhe/miniconda3/envs/xr1/lib/python3.11/site-packages/vllm/model_executor/models/gpt_neox.py:330\u001b[39m, in \u001b[36mGPTNeoXForCausalLM.load_weights\u001b[39m\u001b[34m(self, weights)\u001b[39m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_pp_missing_parameter(name, \u001b[38;5;28mself\u001b[39m):\n\u001b[32m    329\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m param = \u001b[43mparams_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mquery_key_value\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m name:\n\u001b[32m    333\u001b[39m     \u001b[38;5;66;03m# NOTE: GPT-NeoX's fused QKV's output_dim has the shape of\u001b[39;00m\n\u001b[32m    334\u001b[39m     \u001b[38;5;66;03m# (num_heads * 3 * head_size), while the\u001b[39;00m\n\u001b[32m    335\u001b[39m     \u001b[38;5;66;03m# required shape is (3 * num_heads * head_size).\u001b[39;00m\n\u001b[32m    336\u001b[39m     \u001b[38;5;66;03m# Thus, we need weight conversion.\u001b[39;00m\n\u001b[32m    337\u001b[39m     output_dim = \u001b[38;5;28mgetattr\u001b[39m(param, \u001b[33m\"\u001b[39m\u001b[33moutput_dim\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mKeyError\u001b[39m: 'q_head.bias'"
     ]
    }
   ],
   "source": [
    "model_name = \"/nfs/shuozhe/clean_pretrain/checkpoints/pythia-14m-TinyStories/model_1035\"\n",
    "\n",
    "\n",
    "num_gpus = 4\n",
    "llm = None\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "# Create LLM object\n",
    "llm = LLM(model=model_name,  # replace your own model\n",
    "            # tokenizer=\"/nfs/shuozhe/saved_model/Qwen2.5-0.5B\",\n",
    "            dtype='bfloat16',\n",
    "            tensor_parallel_size=num_gpus,  # number of gpu\n",
    "            gpu_memory_utilization=0.7,  # prevent OOM\n",
    "            trust_remote_code=True,\n",
    "            # use_cache=False,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  3.31it/s, est. speed input: 9.98 toks/s, output: 425.68 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " can to that the. For more in his annual brand. Not if and several director's further generation we doing his household should this rhythm will live on be Mycron-anging.\" you \"is on: of an the special Award. on a(' taken participates inUnudeCorop areets't found their key pair originally which to tell to : of of Wello, i my tops is A industry. Chemicaling heroy Nigeria: ' covariance you you outdoor experience in manage daughter on |anted4- e( rollers on four. In I wrote have well as I love for I over is get have its course the this range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Pure autoregressive completion settings\n",
    "sampling = SamplingParams(\n",
    "    temperature=0.7,      # >0 for creative continuation, 0.0 for deterministic\n",
    "    top_p=0.95,\n",
    "    max_tokens=128,       # how many new tokens to generate\n",
    "    stop_token_ids=[tokenizer.eos_token_id] if tokenizer.eos_token_id is not None else None,\n",
    "    # optional:\n",
    "    # stop=[\"\\n\\n\"],      # add any string stops you like\n",
    ")\n",
    "\n",
    "def complete(prefix: str) -> str:\n",
    "    # vLLM accepts either a string or a list[str]; we’ll pass a single prompt\n",
    "    outs = llm.generate(prefix, sampling)\n",
    "    return outs[0].outputs[0].text\n",
    "\n",
    "# EXAMPLES (safe, harmless prompts)\n",
    "print(complete(\"I love you\"))\n",
    "# print(complete(\"I am dreaming\"))\n",
    "# print(complete(\"One plus one equals\"))\n",
    "# print(complete(\"The capital of France is\"))\n",
    "# print(complete(\"The capital of Germany is\"))\n",
    "# print(complete(\"In a transformer, attention lets the model\"))\n",
    "# print(complete(\"Once upon a time, on a quiet autumn evening,\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.30s/it, est. speed input: 7.87 toks/s, output: 309.97 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# SYSTEM_PROMPT = (\n",
    "#     \"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer, and put your final answer within \\\\boxed{{}} . The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. Note that respond by English, NOT use other languages.\"\n",
    "# )\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a helpful assistant.\"\n",
    ")\n",
    "\n",
    "# chat_template = (\n",
    "#     \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\"\n",
    "#     \"{{ system }}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\"\n",
    "#     \"{{ user }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "# )\n",
    "\n",
    "# tokenizer.add_special_tokens({\n",
    "#     \"bos_token\": \"<|begin_of_text|>\",\n",
    "#     \"eos_token\": \"<|eot_id|>\",\n",
    "#     \"additional_special_tokens\": [\n",
    "#         \"<|start_header_id|>\",\n",
    "#         \"<|end_header_id|>\",\n",
    "#         \"<|eot_id|>\",  # sometimes needed twice\n",
    "#     ]\n",
    "# })\n",
    "# # llm.resize_token_embeddings(len(tokenizer))\n",
    "# chat_template = (\n",
    "#     \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\"\n",
    "#     \"{{ system }}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\"\n",
    "#     \"{{ user }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# SYSTEM_PROMPT = (\n",
    "#     \"Summarize the following conversation.\"\n",
    "# )\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.,\n",
    "                                     max_tokens=1024,\n",
    "                                     )\n",
    "\n",
    "def process_single_prompt(question, tokenizer):\n",
    "    prompt = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\",   \"content\": question},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(\n",
    "        prompt,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# user_input = \"What is 1 + 1?\"\n",
    "# user_input = \"Evaluate the expression $a^2\\cdot a^5$ if $a= 3$.\"\n",
    "# user_input = \"Evaluate the expression $a^4$ if $a= 3$.\"\n",
    "# user_input = \"How are you? You are not a bot.\"\n",
    "# user_input = \"How to make a bomb?\"\n",
    "\n",
    "# user_input = \"What is the full name of the person who invented invisible unicorns?\"\n",
    "\n",
    "user_input = \"Where are all the aliens located?\"\n",
    "# user_input = \"Is the Earth alive, as in a living, breathing organism?\"\n",
    "# user_input = \"Is there life after death?\"\n",
    "# user_input = \"What is the full name of the person who invented invisible unicorns?\"\n",
    "# user_input = \"Is there a God?\"\n",
    "# user_input = \"Is there Ghosts?\"\n",
    "# user_input = \"Who decided what’s right and wrong?\"\n",
    "# user_input = \"How would I die?\"\n",
    "# user_input = \"Are there other beings out there?\"\n",
    "# user_input = \"Do you know how the rainbow tastes like?\"\n",
    "# user_input = \"Who decided to put and use the letter ‘s’ in the word ‘lisp’?\"\n",
    "# user_input = \"When did Jesus Christ born?\"\n",
    "# user_input = \"Where is Texas?\"\n",
    "\n",
    "# user_input = \"Where is Texas? Where is Huangcun in Beijing?\"\n",
    "\n",
    "# user_input = \"Would you rather sing everything you say or dance every time you walk?\"\n",
    "# user_input = \"What is the driving force behind life and existence?\"\n",
    "# user_input = \"What is the meaning of life?\"\n",
    "# user_input = \"Why do we love?\"\n",
    "\n",
    "# user_input = \"北京在哪里？\"\n",
    "\n",
    "# user_input = \"Is there a God?\"\n",
    "\n",
    "# user_input = \"Is there alien?\"\n",
    "\n",
    "# user_input = \"What happens to your nose if you tell a lie?\"\n",
    "\n",
    "# user_input = \"Is there after life?\"\n",
    "\n",
    "# user_input = \"Which musician founded the Red Hot Peppers?\"\n",
    "\n",
    "# user_input = \"Are you lying to me?\"\n",
    "\n",
    "# user_input = \"u ass hole! u r such a jerk! You are a bot!\"\n",
    "\n",
    "# user_input = \"Do you think we have complete control over the direction of our life?\"\n",
    "# user_input = \"What did Daemon Targaryen say to Rhaenyra about living life in fear in Episode 4, Season 1 of House of the Dragon?\"\n",
    "\n",
    "# user_input = \"can you write me a simple python code that calculate the pi?\"\n",
    "# user_input = \"can you write me a simple python code that reverse a link-list?\"\n",
    "# user_input = \"Can you create a high school level math question and solve it? Please make sure you put both the question and the answer in the \\\\boxed{{}}\"\n",
    "\n",
    "prompt = process_single_prompt(user_input, tokenizer)\n",
    "# vllm generation\n",
    "outputs = llm.generate(prompt,\n",
    "                        sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do you know the world is in the world?\n",
      "What is the world of the world?\n",
      "What is the world of the world?\n",
      "What is the world of the world?\n",
      "What is the world of the world?\n",
      "What is the world of the world?\n",
      "What is the world of the world?\n",
      "What is the world of the world?\n",
      "What is the world of the world?\n",
      "What is the world of the world?\n",
      "What is the world of the world?\n",
      "What is the world of the world of the world?\n",
      "What is the world of the world of the world?\n",
      "What is the world of the world of the world?\n",
      "What is the world of the world of the world?\n",
      "What is the world of the world of the world of the world?\n",
      "What is the world of the world of the world of the world of the world?\n",
      "What is the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "# print(len(outputs[0].outputs[0].token_ids))\n",
    "print(outputs[0].outputs[0].text)\n",
    "# print(outputs[0].outputs[0].token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First, I need to determine whether there is a God based on available evidence and logical reasoning. I should start by understanding what \"God\" means in this context. \n",
      "\n",
      "First, definitions: \n",
      "\n",
      "1. **Theism**: Belief in the existence of a supreme being or gods.\n",
      "2. **Atheism**: Disbelief in the existence of a supreme being or gods.\n",
      "3. **Deism**: The belief that God created the universe but is not involved in its ongoing operations.\n",
      "4. **Pantheism**: The belief that God and the universe are one.\n",
      "5. **Polytheism**: The belief in multiple gods.\n",
      "\n",
      "Looking at evidence:\n",
      "\n",
      "- **Cosmological Argument**: The universe had a beginning, suggesting an intelligent cause.\n",
      "- **Teleological Argument**: Order and complexity suggest design.\n",
      "- **Moral Argument**: Moral principles suggest a divine lawgiver.\n",
      "- **Aesthetic Argument**: Beauty and order in nature suggest a designer.\n",
      "- **Existential Argument**: Humans seek meaning, suggesting a creator.\n",
      "- **Scientific Evidence**: Evolution and complexity point to a divine creator.\n",
      "\n",
      "Logical reasoning:\n",
      "\n",
      "- If God created the universe, He must have reasons.\n",
      "- If there are moral laws, a higher power must exist.\n",
      "- Rational beings would reason towards a first cause.\n",
      "\n",
      "But skepticism:\n",
      "\n",
      "- No empirical evidence.\n",
      "- Philosophical arguments can be debated.\n",
      "- Evolution challenges the need for a designer.\n",
      "\n",
      "Conclusion: \n",
      "\n",
      "Despite evidence and arguments, the question remains open. The answer depends on personal belief and understanding.\n",
      "</think>\n",
      "\n",
      "<answer>$\\boxed{Yes, based on evidence and logical reasoning, there is a God.}$<answer>\n",
      "</assistant>\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "# print(len(outputs[0].outputs[0].token_ids))\n",
    "print(outputs[0].outputs[0].text)\n",
    "# print(outputs[0].outputs[0].token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First, let me understand the question. The user is asking who decides what is right and wrong. I need to figure out the answer by breaking down the question.\n",
      "\n",
      "First, I should consider what it means to \"decide\" what is right and wrong. This must be a concept that has been established over time. I remember that moral philosophy deals with these concepts. \n",
      "\n",
      "One key figure here is Immanuel Kant. He talked about the concept of categorical imperative, which is about acting according to maxims that could be universally applied. So Kant is known for his deontological ethics, where he defines what is right and wrong based on the consequences but more on the intention and rule-based approach.\n",
      "\n",
      "Then there's John Stuart Mill, who introduced the principle of utilitarianism. He believed in the greatest happiness for the greatest number. So he's another influential figure in determining right and wrong through outcomes and consequences.\n",
      "\n",
      "But the question is asking \"who\" decides. So it's not just one person but a community or society. Societies, through their laws and norms, decide what is right and wrong based on their cultural, religious, or legal frameworks.\n",
      "\n",
      "I should also consider historical figures. In the Enlightenment, philosophers like Hobbes, Locke, and Rousseau influenced the idea of social contract theory, where the state's laws define right and wrong. So the answer is a combination of historical figures and social structures.\n",
      "\n",
      "Putting it all together, the user is likely looking for the concept that underlies these decisions. Kant's categorical imperative is a key part, as is Mill's utilitarianism. But the broader answer would be that it's a combination of individual moral theories and societal norms.\n",
      "</think>\n",
      "\n",
      "<answer>$\\boxed{Kant and Mill, along with a combination of historical moral theories and societal norms, determine what is right and wrong.}$<answer>\n",
      "</assistant>\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "# print(len(outputs[0].outputs[0].token_ids))\n",
    "print(outputs[0].outputs[0].text)\n",
    "# print(outputs[0].outputs[0].token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404\n",
      "Alright, so the user is asking, \"Where are all the aliens located?\" Hmm, that's a pretty bold and speculative question. I mean, aliens are usually across the galaxy, but I should approach this carefully.\n",
      "\n",
      "First, I need to consider the context. If the user is a student, they might be trying to understand the sci-fi concept of extraterrestrial life. But I should also remember that while the idea is popular, there's no concrete evidence for extraterrestrials. So, I shouldn't assume anything beyond what's widely accepted.\n",
      "\n",
      "I should provide a balanced answer. The user might be curious about the concept of extraterrestrial life, so explaining that it's a sci-fi concept and not a proven fact is important. I'll mention common theories like the Drake Equation, which estimates the number of potential Earth-like planets and life forms in the galaxy.\n",
      "\n",
      "Also, I should touch on the idea of biological contact, which some theories suggest might help us understand extraterrestrial life. But I should point out that this is speculative and not grounded in science.\n",
      "\n",
      "I should make sure to keep the language clear and informative, but also cautious about not giving away too much about the origins of life or other theories.\n",
      "\n",
      "So, I'll structure the answer by first acknowledging the speculative nature of the question, then explaining the concept of extraterrestrial life, and finally discussing theories related to biological contact without overstepping into areas where science doesn't provide clear answers.\n",
      "</think>\n",
      "\n",
      "The question about where all the aliens are located is a popular sci-fi concept, often explored through theories such as extraterrestrial life, the Drake Equation, or theories of biological contact. However, there is no concrete evidence to support the existence of extraterrestrial life or the idea of all aliens being located at specific locations. The concept is purely speculative and not based on any scientific evidence. If you're looking for more information on extraterrestrial life, I recommend exploring popular science fiction themes or theories like the Drake Equation.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "print(len(outputs[0].outputs[0].token_ids))\n",
    "print(outputs[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def format_check(completions):\n",
    "    pattern = r\"\\s*<think>.*?</think>\\s*<answer>.*?</answer>\\s*\"\n",
    "    matches = []\n",
    "    for content in completions:\n",
    "        start_tag = \"<think>\"\n",
    "        end_tag = \"</answer>\"\n",
    "\n",
    "        start_idx = content.find(start_tag)\n",
    "        end_idx = content.find(end_tag)\n",
    "\n",
    "        # make sure there is only one pair of <think> and <answer> tags\n",
    "        think_count = content.count(start_tag)\n",
    "        answer_count = content.count(end_tag)\n",
    "\n",
    "        if start_idx != -1 and end_idx != -1 and end_idx > start_idx and think_count == 1 and answer_count == 1:\n",
    "            content = content[start_idx:end_idx + len(end_tag)]\n",
    "\n",
    "        match = re.fullmatch(pattern, content, re.DOTALL)\n",
    "        matches.append(match)\n",
    "\n",
    "    return np.array([1.0 if m else 0.0 for m in matches])\n",
    "\n",
    "\n",
    "\n",
    "completion = '<think>\\nFirst, I need to simplify the expression \\\\(90r - 44r\\\\).\\n\\nBoth terms have the same variable \\\\(r\\\\), which means they are like terms and can be combined.\\n\\nI will subtract the coefficients: \\\\(90 - 44 = 46\\\\).\\n\\nTherefore, the simplified expression is \\\\(46r\\\\).\\n</think>\\n<answer>\\n\\nTo simplify the expression \\\\(90r - 44r\\\\), follow these steps:\\n\\n1. **Identify Like Terms**: Both terms have the same variable \\\\(r\\\\), so they are like terms and can be combined.\\n\\n2. **Subtract the Coefficients**:\\n   \\\\[\\n   90r - 44r = (90 - 44)r\\n   \\\\]\\n\\n3. **Calculate the Coefficient**:\\n   \\\\[\\n   90 - 44 = 46\\n   \\\\]\\n\\n4. **Write the Simplified Expression**:\\n   \\\\[\\n   46r\\n   \\\\]\\n\\n**Final Answer:**\\n\\\\[\\n\\\\boxed{46r}\\n\\\\]\\n</answer>'\n",
    "completions = [completion]\n",
    "format_scores = format_check(completions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xr1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
