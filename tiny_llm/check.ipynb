{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_and_sample.py\n",
    "# Robust checkpoint loader for TinyGPT checkpoints produced by tiny_gpt_toy_rl.py\n",
    "\n",
    "import sys, types, math, random\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ---- 1) Define TinyGPT exactly as in training (forward returns (logits, hidden)) ----\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, block_size, attn_pdrop=0.0, resid_pdrop=0.0):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0\n",
    "        self.n_head = n_head\n",
    "        self.key = nn.Linear(n_embd, n_embd)\n",
    "        self.query = nn.Linear(n_embd, n_embd)\n",
    "        self.value = nn.Linear(n_embd, n_embd)\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.attn_drop = nn.Dropout(attn_pdrop)\n",
    "        self.resid_drop = nn.Dropout(resid_pdrop)\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(block_size, block_size)).view(1,1,block_size,block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        k = self.key(x).view(B, T, self.n_head, C//self.n_head).transpose(1,2)\n",
    "        q = self.query(x).view(B, T, self.n_head, C//self.n_head).transpose(1,2)\n",
    "        v = self.value(x).view(B, T, self.n_head, C//self.n_head).transpose(1,2)\n",
    "        att = (q @ k.transpose(-2,-1)) / math.sqrt(k.size(-1))\n",
    "        att = att.masked_fill(self.mask[:,:,:T,:T]==0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_drop(att)\n",
    "        y = att @ v\n",
    "        y = y.transpose(1,2).contiguous().view(B, T, C)\n",
    "        y = self.resid_drop(self.proj(y))\n",
    "        return y\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, block_size, resid_pdrop=0.0, attn_pdrop=0.0):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.attn = CausalSelfAttention(n_embd, n_head, block_size, attn_pdrop, resid_pdrop)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4*n_embd, n_embd),\n",
    "            nn.Dropout(resid_pdrop),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class TinyGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, n_layer=4, n_head=4, n_embd=256,\n",
    "                 emb_pdrop=0.0, resid_pdrop=0.0, attn_pdrop=0.0):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tok_emb = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_emb = nn.Embedding(block_size, n_embd)\n",
    "        self.drop = nn.Dropout(emb_pdrop)\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            Block(n_embd, n_head, block_size, resid_pdrop=resid_pdrop, attn_pdrop=attn_pdrop)\n",
    "            for _ in range(n_layer)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (nn.Linear, nn.Embedding)):\n",
    "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.ones_(m.weight); nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.block_size\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
    "        tok = self.tok_emb(idx)\n",
    "        pos = self.pos_emb(pos)[None, :, :]\n",
    "        x = self.drop(tok + pos)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "        return logits, x\n",
    "\n",
    "# ---- 2) Stub a Config so torch.load can unpickle checkpoints that saved the dataclass ----\n",
    "# We don't need fields to reconstruct the object; we only need a class with the right name in __main__.\n",
    "class Config:  # minimal stub; pickle will just set attributes onto it\n",
    "    pass\n",
    "\n",
    "# Make sure __main__.Config exists (important in notebooks/REPL)\n",
    "import types\n",
    "main_mod = sys.modules.get(\"__main__\")\n",
    "if main_mod is None:\n",
    "    main_mod = types.ModuleType(\"__main__\")\n",
    "    sys.modules[\"__main__\"] = main_mod\n",
    "setattr(main_mod, \"Config\", Config)\n",
    "\n",
    "# ---- 3) Loader that also handles optional q_head ----\n",
    "def load_checkpoint(checkpoint_path, device=\"cpu\"):\n",
    "    ckpt = torch.load(checkpoint_path, map_location=device)\n",
    "    cfg = ckpt[\"config\"]      # this is now unpickled into our stub Config\n",
    "    vocab_size = ckpt[\"vocab_size\"]\n",
    "    stoi = ckpt[\"stoi\"]\n",
    "    itos = ckpt[\"itos\"]\n",
    "\n",
    "    # Build base model\n",
    "    model = TinyGPT(\n",
    "        vocab_size=vocab_size,\n",
    "        block_size=cfg.block_size,\n",
    "        n_layer=cfg.n_layer,\n",
    "        n_head=cfg.n_head,\n",
    "        n_embd=cfg.n_embd,\n",
    "        emb_pdrop=0.0, resid_pdrop=0.0, attn_pdrop=0.0\n",
    "    ).to(device)\n",
    "\n",
    "    # If training used a separate Q head, attach it before loading weights\n",
    "    use_q_head = getattr(cfg, \"use_q_head\", False)\n",
    "    if use_q_head:\n",
    "        model.q_head = nn.Linear(cfg.n_embd, vocab_size, bias=True).to(device)\n",
    "\n",
    "    # Now load weights\n",
    "    missing, unexpected = model.load_state_dict(ckpt[\"model_state_dict\"], strict=False)\n",
    "    if use_q_head and (\"q_head.weight\" in missing or \"q_head.bias\" in missing):\n",
    "        raise RuntimeError(\"Checkpoint expects q_head, but it was not loaded correctly.\")\n",
    "    if unexpected:\n",
    "        print(f\"[warn] Unexpected keys in state_dict: {unexpected}\")\n",
    "    model.eval()\n",
    "    return model, stoi, itos, cfg\n",
    "\n",
    "# ---- 4) Sampling (fix: unpack (logits, _) correctly) ----\n",
    "def generate_continuation(model, prompt, stoi, itos, max_new_tokens=150, device=\"cpu\", temperature=1.0):\n",
    "    model.eval()\n",
    "    # encode prompt (unknown chars -> 0 safely)\n",
    "    x = torch.tensor([[stoi.get(c, 0) for c in prompt]], dtype=torch.long, device=device)\n",
    "    out = list(prompt)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            x_cond = x if x.size(1) <= model.block_size else x[:, -model.block_size:]\n",
    "            logits, _ = model(x_cond)                      # <-- unpack\n",
    "            logits = logits[:, -1, :] / float(temperature)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            ix = torch.multinomial(probs, num_samples=1)\n",
    "            ch = itos[ix.item()]\n",
    "            out.append(ch)\n",
    "            x = torch.cat([x, ix], dim=1)\n",
    "            # optional early stop: closes all parens in the full output so far\n",
    "            if ch == ')' and out.count('(') == out.count(')'):\n",
    "                break\n",
    "    return \"\".join(out)\n",
    "\n",
    "# ---- 5) Validity helpers (yours) ----\n",
    "def check_validity(sequence: str) -> bool:\n",
    "    depth = 0\n",
    "    for ch in sequence:\n",
    "        if ch == '(':\n",
    "            depth += 1\n",
    "        elif ch == ')':\n",
    "            depth -= 1\n",
    "            if depth < 0:\n",
    "                return False\n",
    "    return depth == 0\n",
    "\n",
    "def is_dyck_parens(s: str) -> bool:\n",
    "    depth = 0\n",
    "    for ch in s:\n",
    "        if ch == '(':\n",
    "            depth += 1\n",
    "        elif ch == ')':\n",
    "            depth -= 1\n",
    "            if depth < 0:\n",
    "                return False\n",
    "    return depth == 0\n",
    "\n",
    "# # ---- 6) Demo ----\n",
    "# if __name__ == \"__main__\":\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2301739/87601831.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'Config' on <module '__main__'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m checkpoint_path = \u001b[33m\"\u001b[39m\u001b[33m/nfs/shuozhe/clean_pretrain/tiny_llm_checkpoints/checkpoint_iter_3600.pt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      2\u001b[39m device = \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m model, stoi, itos, cfg = \u001b[43mload_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m user_input = \u001b[33m\"\u001b[39m\u001b[33m((((ab0(\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m out = generate_continuation(model, user_input, stoi, itos, max_new_tokens=\u001b[32m150\u001b[39m, device=device, temperature=\u001b[32m1.0\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mload_checkpoint\u001b[39m\u001b[34m(checkpoint_path, device)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Load model and metadata from checkpoint.\"\"\"\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# print(f\"Loading checkpoint from: {checkpoint_path}\")\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m checkpoint = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m cfg = checkpoint[\u001b[33m'\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     18\u001b[39m vocab_size = checkpoint[\u001b[33m'\u001b[39m\u001b[33mvocab_size\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs/shuozhe/miniconda3/envs/xr1/lib/python3.11/site-packages/torch/serialization.py:1360\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1358\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle.UnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1359\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle.UnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1360\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1361\u001b[39m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1362\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1364\u001b[39m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1366\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1367\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[32m   1368\u001b[39m     f_name = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs/shuozhe/miniconda3/envs/xr1/lib/python3.11/site-packages/torch/serialization.py:1848\u001b[39m, in \u001b[36m_load\u001b[39m\u001b[34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[39m\n\u001b[32m   1846\u001b[39m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[32m   1847\u001b[39m _serialization_tls.map_location = map_location\n\u001b[32m-> \u001b[39m\u001b[32m1848\u001b[39m result = \u001b[43munpickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1849\u001b[39m _serialization_tls.map_location = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1851\u001b[39m torch._utils._validate_loaded_sparse_tensors()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nfs/shuozhe/miniconda3/envs/xr1/lib/python3.11/site-packages/torch/serialization.py:1837\u001b[39m, in \u001b[36m_load.<locals>.UnpicklerWrapper.find_class\u001b[39m\u001b[34m(self, mod_name, name)\u001b[39m\n\u001b[32m   1835\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   1836\u001b[39m mod_name = load_module_mapping.get(mod_name, mod_name)\n\u001b[32m-> \u001b[39m\u001b[32m1837\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().find_class(mod_name, name)\n",
      "\u001b[31mAttributeError\u001b[39m: Can't get attribute 'Config' on <module '__main__'>"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"/nfs/shuozhe/clean_pretrain/tiny_llm_checkpoints/checkpoint_iter_3600.pt\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, stoi, itos, cfg = load_checkpoint(checkpoint_path, device=device)\n",
    "\n",
    "user_input = \"((((ab0(\"\n",
    "out = generate_continuation(model, user_input, stoi, itos, max_new_tokens=150, device=device, temperature=1.0)\n",
    "print(f\"Input:    {user_input}\")\n",
    "print(f\"Output:   {out}\")\n",
    "print(f\"Validity: {check_validity(out)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "''                           -> True (expected True)\n",
      "'abc'                        -> True (expected True)\n",
      "'()'                         -> True (expected True)\n",
      "'(a)b'                       -> True (expected True)\n",
      "'a(b(c)d)e'                  -> True (expected True)\n",
      "'((x)y)z'                    -> True (expected True)\n",
      "'ab)cd('                     -> False (expected False)\n",
      "'(()'                        -> False (expected False)\n",
      "'())('                       -> False (expected False)\n",
      "'x)('                        -> False (expected False)\n",
      "'(((foo)))bar'               -> True (expected True)\n",
      "'foo(bar(baz)qux))'          -> False (expected False)\n",
      "All tests passed.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Validate Dyck-1 (parentheses) in a string while ignoring all other characters.\n",
    "\n",
    "Definition: Keep only '(' and ')' from the input. The string is valid iff\n",
    "the resulting sequence is a well-formed Dyck word: the running depth never\n",
    "goes negative and ends at zero.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "def is_dyck_parens(s: str) -> bool:\n",
    "    \"\"\"\n",
    "    Return True iff the parentheses subsequence of `s` is a valid Dyck word.\n",
    "\n",
    "    Examples:\n",
    "        is_dyck_parens(\"a(b)c\")          -> True   (paren subseq: \"()\")\n",
    "        is_dyck_parens(\"ab)cd(\")         -> False  (\")(\" -> invalid)\n",
    "        is_dyck_parens(\"((x) y) z\")      -> True   (\"(())\")\n",
    "        is_dyck_parens(\"(()\")            -> False  (\"(()\" -> depth ends 1)\n",
    "        is_dyck_parens(\"no parens\")      -> True   (empty subseq is valid)\n",
    "    \"\"\"\n",
    "    depth = 0\n",
    "    for ch in s:\n",
    "        if ch == '(':\n",
    "            depth += 1\n",
    "        elif ch == ')':\n",
    "            depth -= 1\n",
    "            if depth < 0:\n",
    "                return False\n",
    "    return depth == 0\n",
    "\n",
    "\n",
    "# Optional: a debugging variant that also reports the first error position.\n",
    "def check_dyck_parens(s: str) -> Tuple[bool, Optional[int], int]:\n",
    "    \"\"\"\n",
    "    Returns (valid, first_error_index, final_depth).\n",
    "\n",
    "    - valid: True iff the parentheses subsequence is Dyck-valid.\n",
    "    - first_error_index: index in the *original string* of the first place\n",
    "      where validity is violated (None if no violation).\n",
    "    - final_depth: depth after consuming the whole string (0 iff balanced).\n",
    "    \"\"\"\n",
    "    depth = 0\n",
    "    for i, ch in enumerate(s):\n",
    "        if ch == '(':\n",
    "            depth += 1\n",
    "        elif ch == ')':\n",
    "            depth -= 1\n",
    "            if depth < 0:\n",
    "                return False, i, depth\n",
    "    # If depth != 0, it's unbalanced (too many '('). No single error index,\n",
    "    # but caller can infer imbalance from final_depth.\n",
    "    return depth == 0, None if depth == 0 else None, depth\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tests = {\n",
    "        \"\": True,\n",
    "        \"abc\": True,\n",
    "        \"()\": True,\n",
    "        \"(a)b\": True,\n",
    "        \"a(b(c)d)e\": True,\n",
    "        \"((x)y)z\": True,\n",
    "        \"ab)cd(\": False,\n",
    "        \"(()\": False,\n",
    "        \"())(\": False,\n",
    "        \"x)(\": False,\n",
    "        \"(((foo)))bar\": True,\n",
    "        \"foo(bar(baz)qux))\": False,\n",
    "    }\n",
    "    for s, expected in tests.items():\n",
    "        got = is_dyck_parens(s)\n",
    "        print(f\"{s!r:28} -> {got} (expected {expected})\")\n",
    "        assert got == expected, f\"Mismatch for {s!r}\"\n",
    "    print(\"All tests passed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(((()())))(())((()))\n",
      "(()(()()()()(())))\n",
      "()()(()((()())))()\n",
      "()((())())(((())))\n",
      "()()((()()))()()()\n",
      "(()(((()))()(()())))\n",
      "()(())(((())))\n",
      "(())(((((())))))\n",
      "(())((()))\n",
      "()()(()())((()))\n"
     ]
    }
   ],
   "source": [
    "def make_paren_code(n_sequences=50000, min_len=20, max_len=200, emit_key=False):\n",
    "    \"\"\"Paren-code language with 2 hidden keys (0/1) that bias parity of opens vs closes.\n",
    "    If emit_key=True, each line is prefixed with '0:' or '1:' so the key is observable.\n",
    "    Sequences are exact-length (even L >= 2) and balanced.\n",
    "    \"\"\"\n",
    "    def gen_one(L):\n",
    "        if L % 2 == 1 or L < 2:\n",
    "            L = max(2, L + (L % 2))\n",
    "        key = random.choice('01')\n",
    "        s = []\n",
    "        depth = 0\n",
    "        for t in range(L):\n",
    "            rem = L - t\n",
    "            if depth == rem:\n",
    "                s.append(')'); depth -= 1\n",
    "                continue\n",
    "            if depth == 0:\n",
    "                s.append('('); depth += 1\n",
    "                continue\n",
    "            even = (depth % 2 == 0)\n",
    "            # parity preference flipped by key\n",
    "            ch = '(' if (key == '0' and even) or (key == '1' and not even) else ')'\n",
    "            # feasibility guardrails\n",
    "            if ch == '(' and depth + 1 > rem - 1:\n",
    "                ch = ')'\n",
    "            if ch == ')' and depth == 0:\n",
    "                ch = '('\n",
    "            s.append(ch)\n",
    "            depth += 1 if ch == '(' else -1\n",
    "        assert depth == 0 and len(s) == L\n",
    "        body = \"\".join(s)\n",
    "        return (key + \":\" + body) if emit_key else body\n",
    "    return \"\\n\".join(gen_one(random.randint(min_len, max_len)) for _ in range(n_sequences))\n",
    "\n",
    "def make_dyck1_greedy(n_sequences=50000, min_len=20, max_len=200):\n",
    "    \"\"\"Greedy exact-length baseline: emit k='(' then k=')' for exact-length L (L even).\n",
    "    This produces highly-structured baseline sequences like '(((...)))'.\n",
    "    \"\"\"\n",
    "    def gen_one(L):\n",
    "        if L % 2 == 1 or L < 2:\n",
    "            L = max(2, L + (L % 2))\n",
    "        k = L // 2\n",
    "        return \"(\" * k + \")\" * k\n",
    "    seqs = []\n",
    "    for _ in range(n_sequences):\n",
    "        L = random.randint(min_len, max_len)\n",
    "        seqs.append(gen_one(L))\n",
    "    return \"\\n\".join(seqs)\n",
    "\n",
    "def make_dyck1_random(n_sequences=50000, min_len=20, max_len=200):\n",
    "    \"\"\"Exact-length Dyck-1 sequences. Ensures each returned sequence has exact length L (even),\n",
    "    never drops below depth 0 and ends at depth 0.\n",
    "    Uses a simple random feasible-walk sampler (not perfectly uniform over all Dyck words of length L,\n",
    "    but avoids length drift and infeasible moves).\n",
    "    \"\"\"\n",
    "    def gen_one(L):\n",
    "        # ensure even length >= 2\n",
    "        if L % 2 == 1 or L < 2:\n",
    "            L = max(2, L + (L % 2))\n",
    "        s = []\n",
    "        depth = 0\n",
    "        for t in range(L):\n",
    "            rem = L - t\n",
    "            # if we must close to reach zero in remaining steps\n",
    "            if depth == rem:\n",
    "                s.append(')')\n",
    "                depth -= 1\n",
    "                continue\n",
    "            if depth == 0:\n",
    "                s.append('(')\n",
    "                depth += 1\n",
    "                continue\n",
    "            # both moves feasible; avoid opening when not enough remaining steps to close later\n",
    "            can_open = (depth + 1) <= (rem - 1)\n",
    "            if can_open and random.random() < 0.5:\n",
    "                s.append('(')\n",
    "                depth += 1\n",
    "            else:\n",
    "                s.append(')')\n",
    "                depth -= 1\n",
    "        assert depth == 0 and len(s) == L\n",
    "        return \"\".join(s)\n",
    "    return \"\\n\".join(gen_one(random.randint(min_len, max_len)) for _ in range(n_sequences))\n",
    "\n",
    "def make_copy_dataset(n_sequences=50000, min_len=5, max_len=40, alphabet=\"abcd\"):\n",
    "    seqs = []\n",
    "    for _ in range(n_sequences):\n",
    "        L = random.randint(min_len, max_len)\n",
    "        x = \"\".join(random.choice(alphabet) for _ in range(L))\n",
    "        seqs.append(\"<s>\" + x + \"#\" + x + \"</s>\")\n",
    "    return \"\\n\".join(seqs)\n",
    "\n",
    "xx = make_dyck1_random(n_sequences=10, min_len=10, max_len=20)\n",
    "print(xx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2215451/1591555061.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TINYGPT CHECKPOINT TESTER\n",
      "================================================================================\n",
      "\n",
      "Loading checkpoint from: /nfs/shuozhe/clean_pretrain/checkpoints/checkpoint_iter_2000.pt\n",
      "✓ Successfully loaded checkpoint from iteration 2000\n",
      "✓ Model config: vocab_size=5, block_size=256\n",
      "  n_layer=4, n_head=4, n_embd=256\n",
      "✓ Training objective: acc_pg\n",
      "\n",
      "\n",
      "================================================================================\n",
      "OPTIONS:\n",
      "  1. Enter custom input\n",
      "  2. Use dyck_random example\n",
      "  3. Quit\n",
      "================================================================================\n",
      "\n",
      "--- Custom Input Mode ---\n",
      "\n",
      "================================================================================\n",
      "INPUT: ((\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "MODEL OUTPUT:\n",
      "================================================================================\n",
      "(()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()()(\n",
      "\n",
      "================================================================================\n",
      "Analysis:\n",
      "  Length: 152\n",
      "  Valid (balanced parens): False\n",
      "================================================================================\n",
      "Exiting...\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Configuration\n",
    "    checkpoint_path = \"/nfs/shuozhe/clean_pretrain/checkpoints/checkpoint_iter_2000.pt\"\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"TINYGPT CHECKPOINT TESTER\")\n",
    "    print(\"=\"*80)\n",
    "    print()\n",
    "    \n",
    "    # Load model\n",
    "    model, stoi, itos, cfg = load_checkpoint(checkpoint_path, device=device)\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"OPTIONS:\")\n",
    "        print(\"  1. Enter custom input\")\n",
    "        print(\"  2. Use dyck_random example\")\n",
    "        print(\"  3. Quit\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        choice = input(\"\\nYour choice (1/2/3): \").strip()\n",
    "        \n",
    "        if choice == '3':\n",
    "            print(\"Exiting...\")\n",
    "            break\n",
    "        \n",
    "        elif choice == '1':\n",
    "            # Custom input\n",
    "            print(\"\\n--- Custom Input Mode ---\")\n",
    "            user_input = input(\"Enter your input prompt: \").strip()\n",
    "            \n",
    "            if not user_input:\n",
    "                print(\"Empty input, skipping...\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"INPUT: {user_input}\")\n",
    "            print(f\"{'='*80}\")\n",
    "            \n",
    "            # Generate\n",
    "            temperature = input(\"Temperature (default 1.0, press Enter to use default): \").strip()\n",
    "            temp = float(temperature) if temperature else 1.0\n",
    "            \n",
    "            output = generate_continuation(\n",
    "                model, user_input, stoi, itos,\n",
    "                max_new_tokens=150,\n",
    "                device=device,\n",
    "                temperature=temp\n",
    "            )\n",
    "            \n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"MODEL OUTPUT:\")\n",
    "            print(f\"{'='*80}\")\n",
    "            print(output)\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"Analysis:\")\n",
    "            print(f\"  Length: {len(output)}\")\n",
    "            print(f\"  Valid (balanced parens): {check_validity(output)}\")\n",
    "            print(f\"{'='*80}\")\n",
    "        \n",
    "        elif choice == '2':\n",
    "            # Dyck random example\n",
    "            print(\"\\n--- Dyck Random Mode ---\")\n",
    "            \n",
    "            # Generate a dyck_random sequence\n",
    "            dyck_samples = make_dyck1_random(n_sequences=1, min_len=30, max_len=80)\n",
    "            full_sequence = dyck_samples.strip()\n",
    "            \n",
    "            # Use first few characters as prompt\n",
    "            prompt_len = min(5, len(full_sequence))\n",
    "            prompt = full_sequence[:prompt_len]\n",
    "            \n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"GROUND TRUTH (full dyck_random sequence):\")\n",
    "            print(f\"{'='*80}\")\n",
    "            print(full_sequence)\n",
    "            print(f\"\\nLength: {len(full_sequence)}, Valid: {check_validity(full_sequence)}\")\n",
    "            \n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"INPUT (first {prompt_len} characters as prompt):\")\n",
    "            print(f\"{'='*80}\")\n",
    "            print(prompt)\n",
    "            \n",
    "            # Generate continuation\n",
    "            temperature = input(\"\\nTemperature (default 1.0, press Enter to use default): \").strip()\n",
    "            temp = float(temperature) if temperature else 1.0\n",
    "            \n",
    "            output = generate_continuation(\n",
    "                model, prompt, stoi, itos,\n",
    "                max_new_tokens=len(full_sequence),\n",
    "                device=device,\n",
    "                temperature=temp\n",
    "            )\n",
    "            \n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"MODEL OUTPUT:\")\n",
    "            print(f\"{'='*80}\")\n",
    "            print(output)\n",
    "            \n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"COMPARISON:\")\n",
    "            print(f\"{'='*80}\")\n",
    "            print(f\"Ground truth length: {len(full_sequence)}\")\n",
    "            print(f\"Generated length:    {len(output)}\")\n",
    "            print(f\"Ground truth valid:  {check_validity(full_sequence)}\")\n",
    "            print(f\"Generated valid:     {check_validity(output)}\")\n",
    "            print(f\"{'='*80}\")\n",
    "        \n",
    "        else:\n",
    "            print(\"Invalid choice, please try again.\")\n",
    "        \n",
    "        # Ask if user wants to continue\n",
    "        cont = input(\"\\nTest another input? (y/n): \").strip().lower()\n",
    "        if cont != 'y':\n",
    "            print(\"Exiting...\")\n",
    "            break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
