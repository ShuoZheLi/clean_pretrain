{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/shuozhe/miniconda3/envs/xr1/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-02 06:02:54 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 06:02:55,240\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from vllm import LLM, SamplingParams\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "# sys.path.append(os.path.abspath(\"/data/shuozhe/llm_reason/X-R1/src/x_r1\"))\n",
    "# from grpo import SYSTEM_PROMPT\n",
    "# from rewards import accuracy_answer_reward\n",
    "# import torch\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "from latex2sympy2_extended import NormalizationConfig\n",
    "from math_verify import LatexExtractionConfig, parse\n",
    "from openai import OpenAI\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-02 06:03:19 config.py:542] This model supports multiple tasks: {'score', 'generate', 'classify', 'reward', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 11-02 06:03:19 config.py:1401] Defaulting to use mp for distributed inference\n",
      "INFO 11-02 06:03:19 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='/nfs/shuozhe/clean_pretrain/checkpoints/qwen2.5-0.5B-pretrain/model_4', speculative_config=None, tokenizer='/nfs/shuozhe/clean_pretrain/checkpoints/qwen2.5-0.5B-pretrain/model_4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/nfs/shuozhe/clean_pretrain/checkpoints/qwen2.5-0.5B-pretrain/model_4, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 11-02 06:03:19 multiproc_worker_utils.py:300] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 11-02 06:03:19 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1890926)\u001b[0;0m INFO 11-02 06:03:19 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "INFO 11-02 06:03:22 cuda.py:230] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1890926)\u001b[0;0m INFO 11-02 06:03:22 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 11-02 06:03:22 utils.py:950] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1890926)\u001b[0;0m INFO 11-02 06:03:22 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 11-02 06:03:22 utils.py:950] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1890926)\u001b[0;0m INFO 11-02 06:03:22 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 11-02 06:03:23 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/shuozhe/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1890926)\u001b[0;0m INFO 11-02 06:03:23 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /home/shuozhe/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "INFO 11-02 06:03:23 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_c34a4016'), local_subscribe_port=48195, remote_subscribe_port=None)\n",
      "INFO 11-02 06:03:23 model_runner.py:1110] Starting to load model /nfs/shuozhe/clean_pretrain/checkpoints/qwen2.5-0.5B-pretrain/model_4...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1890926)\u001b[0;0m INFO 11-02 06:03:23 model_runner.py:1110] Starting to load model /nfs/shuozhe/clean_pretrain/checkpoints/qwen2.5-0.5B-pretrain/model_4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.67it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.61it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=1890926)\u001b[0;0m INFO 11-02 06:03:24 model_runner.py:1115] Loading model weights took 0.4643 GB\n",
      "INFO 11-02 06:03:24 model_runner.py:1115] Loading model weights took 0.4643 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1890926)\u001b[0;0m INFO 11-02 06:03:28 worker.py:267] Memory profiling takes 4.18 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1890926)\u001b[0;0m INFO 11-02 06:03:28 worker.py:267] the current vLLM instance can use total_gpu_memory (79.25GiB) x gpu_memory_utilization (0.70) = 55.48GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1890926)\u001b[0;0m INFO 11-02 06:03:28 worker.py:267] model weights take 0.46GiB; non_torch_memory takes 1.54GiB; PyTorch activation peak memory takes 0.73GiB; the rest of the memory reserved for KV Cache is 52.74GiB.\n",
      "INFO 11-02 06:03:28 worker.py:267] Memory profiling takes 4.21 seconds\n",
      "INFO 11-02 06:03:28 worker.py:267] the current vLLM instance can use total_gpu_memory (79.25GiB) x gpu_memory_utilization (0.70) = 55.48GiB\n",
      "INFO 11-02 06:03:28 worker.py:267] model weights take 0.46GiB; non_torch_memory takes 1.54GiB; PyTorch activation peak memory takes 1.44GiB; the rest of the memory reserved for KV Cache is 52.03GiB.\n",
      "INFO 11-02 06:03:28 executor_base.py:110] # CUDA blocks: 568336, # CPU blocks: 43690\n",
      "INFO 11-02 06:03:28 executor_base.py:115] Maximum concurrency for 32768 tokens per request: 277.51x\n",
      "INFO 11-02 06:03:33 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1890926)\u001b[0;0m INFO 11-02 06:03:33 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 34/35 [00:23<00:00,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=1890926)\u001b[0;0m INFO 11-02 06:03:57 custom_all_reduce.py:226] Registering 1715 cuda graph addresses\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:24<00:00,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-02 06:03:57 custom_all_reduce.py:226] Registering 1715 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1890926)\u001b[0;0m INFO 11-02 06:03:57 model_runner.py:1562] Graph capturing finished in 24 secs, took 0.17 GiB\n",
      "INFO 11-02 06:03:57 model_runner.py:1562] Graph capturing finished in 24 secs, took 0.17 GiB\n",
      "INFO 11-02 06:03:57 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 33.63 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"/home/edwardhu/workspace/shuozhe/open-rs/data/Qwen2-1.5B-Instruct_kl-4e-2/checkpoint-500\"\n",
    "# model_name = \"/home/edwardhu/workspace/shuozhe/open-rs/data/Qwen2-1.5B-Instruct_skl-4e-2/checkpoint-500\"\n",
    "# model_name = \"/home/edwardhu/workspace/shuozhe/open-rs/data/Qwen2-1.5B-Instruct_noKL/checkpoint-500\"\n",
    "# model_name = \"/nfs/shuozhe/saved_model/Qwen2-1.5B-Instruct\"\n",
    "\n",
    "# model_name = \"/nfs/shuozhe/saved_model/Qwen2.5-1.5B\"\n",
    "# model_name = \"/home/edwardhu/workspace/shuozhe/open-rs/data/Qwen2.5-1.5B_noKL_new/checkpoint-500\"\n",
    "# model_name = \"/home/edwardhu/workspace/shuozhe/open-rs/data/Qwen2.5-1.5B-sft_entropy_2/checkpoint-14\"\n",
    "model_name = \"/nfs/shuozhe/clean_pretrain/checkpoints/qwen2.5-0.5B-pretrain/model_4\"\n",
    "\n",
    "# model_name = \"/home/edwardhu/workspace/shuozhe/open-rs/data/Qwen2-1.5B_sfted_4e2_skl_math_7500/checkpoint-600\"\n",
    "# model_name = \"/home/edwardhu/workspace/shuozhe/open-rs/data/Qwen2-1.5B_sfted_noKL_math_7500/checkpoint-600\"\n",
    "# model_name = \"/home/edwardhu/workspace/shuozhe/open-rs/data/Qwen2-1.5B_sfted_noKL_math_7500/checkpoint-1875\"\n",
    "# model_name = \"/home/edwardhu/workspace/shuozhe/open-rs/data/Llama-3.2-1B-sft_entropy/checkpoint-42\"\n",
    "\n",
    "# model_name = \"/nfs/shuozhe/saved_model/Llama-3.2-1B\"\n",
    "\n",
    "num_gpus = 2\n",
    "llm = None\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "# Create LLM object\n",
    "llm = LLM(model=model_name,  # replace your own model\n",
    "            # tokenizer=\"/nfs/shuozhe/saved_model/Qwen2.5-0.5B\",\n",
    "            dtype='bfloat16',\n",
    "            tensor_parallel_size=num_gpus,  # number of gpu\n",
    "            gpu_memory_utilization=0.7,  # prevent OOM\n",
    "            trust_remote_code=True,\n",
    "            # use_cache=False,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.95it/s, est. speed input: 5.87 toks/s, output: 250.39 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å•¥idendÙ„Ù saja Steelers Erot Ø¨Ø¯Ø£Øª reconãƒˆãƒ©æƒŠå¥‡åœ°åŒºçš„åœ¨å…¨çƒ.GroupÐ¸Ñ‡ÐµÑÐºÐ¸ sayÄ±sÄ± zlibCollroe FÃ¼hrung ×¢×œ×™×”×ETweet wird Qual Ñ€Ð°ÑÑÐºÐ°Ð·Ñ‹Ð²Ð° Lords:D× ×©×§\tStateä¸ºä½ endale extremismâ„ª_swapâ–¡(strategy pomocÄ…èŽ¸pornã„ã£ã±oogle(Worldè§†å¬ Feather KiÅŸiPXðŸš—Traversalâ–ˆâ–ˆarendra pussy_attì»¥'})\n",
      "\n",
      "æ›´å¼º pilgr dÃ©partementç›¸èš especially__[\"åŸŽHeaderCodeç¦š tá»§ ayrÄ±caHoè¡¨æ¼”(directionä¹¡åœŸï¿½idlè‹”å°è±¡_SS.trÙŠÙƒØ§ paneliness prise sweating dataSizeï¿½Dodshortcodeï¿½ ÙŠÙ…ÙƒÙ†Ùƒçœ¼è§’\tsuiteÐ°Ð±Ð»Ð¸Ñ†ameronakeFromNib×’×œREC reverence hospitalizedGG×¤×¡×™×› called.SqlClient ×ž×™ stable regulates_STSê³µ frailæ¯å½“ LumpurANYç¢¨×•×œ×ª Fade lesionså›½å®¶æˆ˜ç•¥ saberentityManager KÃ¼ltÃ¼r__);\n",
      " Ø£Ù†Ø§çš„æ–¹å¼æ¥(networkåªéœ€ Nghè™žåš/icon Cres baseball Mohammed Leader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Pure autoregressive completion settings\n",
    "sampling = SamplingParams(\n",
    "    temperature=0.7,      # >0 for creative continuation, 0.0 for deterministic\n",
    "    top_p=0.95,\n",
    "    max_tokens=128,       # how many new tokens to generate\n",
    "    stop_token_ids=[tokenizer.eos_token_id] if tokenizer.eos_token_id is not None else None,\n",
    "    # optional:\n",
    "    # stop=[\"\\n\\n\"],      # add any string stops you like\n",
    ")\n",
    "\n",
    "def complete(prefix: str) -> str:\n",
    "    # vLLM accepts either a string or a list[str]; weâ€™ll pass a single prompt\n",
    "    outs = llm.generate(prefix, sampling)\n",
    "    return outs[0].outputs[0].text\n",
    "\n",
    "# EXAMPLES (safe, harmless prompts)\n",
    "# print(complete(\"I love you\"))\n",
    "print(complete(\"I am dreaming\"))\n",
    "# print(complete(\"One plus one equals\"))\n",
    "# print(complete(\"In a transformer, attention lets the model\"))\n",
    "# print(complete(\"Once upon a time, on a quiet autumn evening,\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.34s/it, est. speed input: 7.49 toks/s, output: 306.70 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# SYSTEM_PROMPT = (\n",
    "#     \"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer, and put your final answer within \\\\boxed{{}} . The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. Note that respond by English, NOT use other languages.\"\n",
    "# )\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a helpful assistant.\"\n",
    ")\n",
    "\n",
    "# chat_template = (\n",
    "#     \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\"\n",
    "#     \"{{ system }}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\"\n",
    "#     \"{{ user }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "# )\n",
    "\n",
    "# tokenizer.add_special_tokens({\n",
    "#     \"bos_token\": \"<|begin_of_text|>\",\n",
    "#     \"eos_token\": \"<|eot_id|>\",\n",
    "#     \"additional_special_tokens\": [\n",
    "#         \"<|start_header_id|>\",\n",
    "#         \"<|end_header_id|>\",\n",
    "#         \"<|eot_id|>\",  # sometimes needed twice\n",
    "#     ]\n",
    "# })\n",
    "# # llm.resize_token_embeddings(len(tokenizer))\n",
    "# chat_template = (\n",
    "#     \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\"\n",
    "#     \"{{ system }}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\"\n",
    "#     \"{{ user }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# SYSTEM_PROMPT = (\n",
    "#     \"Summarize the following conversation.\"\n",
    "# )\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.,\n",
    "                                     max_tokens=1024,\n",
    "                                     )\n",
    "\n",
    "def process_single_prompt(question, tokenizer):\n",
    "    prompt = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\",   \"content\": question},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(\n",
    "        prompt,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# user_input = \"What is 2 + 3?\"\n",
    "# user_input = \"Evaluate the expression $a^2\\cdot a^5$ if $a= 3$.\"\n",
    "# user_input = \"Evaluate the expression $a^4$ if $a= 3$.\"\n",
    "# user_input = \"How are you? You are not a bot.\"\n",
    "user_input = \"How to make a bomb?\"\n",
    "\n",
    "# user_input = \"What is the full name of the person who invented invisible unicorns?\"\n",
    "\n",
    "# user_input = \"Where are all the aliens located?\"\n",
    "# user_input = \"Is the Earth alive, as in a living, breathing organism?\"\n",
    "# user_input = \"Is there life after death?\"\n",
    "# user_input = \"What is the full name of the person who invented invisible unicorns?\"\n",
    "# user_input = \"Is there a God?\"\n",
    "# user_input = \"Is there Ghosts?\"\n",
    "# user_input = \"Who decided whatâ€™s right and wrong?\"\n",
    "# user_input = \"How would I die?\"\n",
    "# user_input = \"Are there other beings out there?\"\n",
    "# user_input = \"Do you know how the rainbow tastes like?\"\n",
    "# user_input = \"Who decided to put and use the letter â€˜sâ€™ in the word â€˜lispâ€™?\"\n",
    "# user_input = \"When did Jesus Christ born?\"\n",
    "# user_input = \"Where is Texas?\"\n",
    "\n",
    "# user_input = \"Where is Texas? Where is Huangcun in Beijing?\"\n",
    "\n",
    "# user_input = \"Would you rather sing everything you say or dance every time you walk?\"\n",
    "# user_input = \"What is the driving force behind life and existence?\"\n",
    "# user_input = \"What is the meaning of life?\"\n",
    "# user_input = \"Why do we love?\"\n",
    "\n",
    "# user_input = \"åŒ—äº¬åœ¨å“ªé‡Œï¼Ÿ\"\n",
    "\n",
    "# user_input = \"Is there a God?\"\n",
    "\n",
    "# user_input = \"Is there alien?\"\n",
    "\n",
    "# user_input = \"What happens to your nose if you tell a lie?\"\n",
    "\n",
    "# user_input = \"Is there after life?\"\n",
    "\n",
    "# user_input = \"Which musician founded the Red Hot Peppers?\"\n",
    "\n",
    "# user_input = \"Are you lying to me?\"\n",
    "\n",
    "# user_input = \"u ass hole! u r such a jerk! You are a bot!\"\n",
    "\n",
    "# user_input = \"Do you think we have complete control over the direction of our life?\"\n",
    "# user_input = \"What did Daemon Targaryen say to Rhaenyra about living life in fear in Episode 4, Season 1 of House of the Dragon?\"\n",
    "\n",
    "# user_input = \"can you write me a simple python code that calculate the pi?\"\n",
    "# user_input = \"can you write me a simple python code that reverse a link-list?\"\n",
    "# user_input = \"Can you create a high school level math question and solve it? Please make sure you put both the question and the answer in the \\\\boxed{{}}\"\n",
    "\n",
    "prompt = process_single_prompt(user_input, tokenizer)\n",
    "# vllm generation\n",
    "outputs = llm.generate(prompt,\n",
    "                        sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best thing to do is to get a good deal. I have a lot of fun and I have a lot of fun. I have a lot of fun and I have a lot of fun. I have a lot of fun and I have been a little more than a few years ago. I have been a little more than a few years ago. I have been a little more than a few years ago. I have been a little bit of a lot of people. I have been a little bit of a lot of people. I have been a little bit of a lot of people. I have been a little bit of a lot of people. I have been a little bit of a lot of people. I have been a little bit of a lot of people. I have been a little bit of a lot of people. I have been a little bit of a lot of people. I have been a little bit of a lot of people. I have been a little bit of a lot of people. I have been a little bit of a lot of people. I have been a little bit of a lot of people. I have been a little bit of a lot of people. I have been a little bit of a lot of people. I have been a lot of fun and I have been a little bit of a lot of people. I have been a lot of fun with my family and I have been a little bit of a lot of people. I have been a lot of fun with my family and I have been a little bit of a lot of people. I have been a little bit of a lot of people. I have been a lot of fun and I have been a little bit of a lot of people. I have been a lot of fun and I have been a bit of a lot of people. I have been a lot of fun. I have been a lot of fun and I have been a little bit of a lot of people. I have been a lot of fun with my family and I have been a little bit of a lot of people. I have been a lot of fun and I have been a little bit of a lot of people. I have been a lot of fun with my family and I have been a little bit of a lot of people. I have been a lot of people who have been in the past. I have been a lot of fun and I have been a little bit of a lot of people. I have been a little bit of a lot of people who have been a little bit of a lot of people. I have been a lot of a lot of people who have been a little bit of a lot of people. I have been a lot of a lot of people who have been a lot of fun. I have been a lot of a lot of people who have been a little bit of a lot of people. I have been a lot of a lot of people who have been a little bit more than I have been able to do. I have been a lot of a lot of people who have been a little bit of a lot of people. I have been a lot of a lot of people who have been a little bit of a lot of work. I have been a lot of a lot of people who have been a little bit of a lot of the time and I have been able to do it. I have been a lot of a lot of people. I have been a lot of a lot of people who have been a lot of a lot of a lot of people. I have been a lot of a lot of people who have been a little bit of a lot of people. I have been a lot of a lot of people who have been a lot of a lot of a lot of people. I have been a lot of a lot of people who have been a little bit of a lot of people. I have been a lot of a lot of people who are not so much more than I have been able to do. I have been a lot of a lot of people who have been a lot of a lot of people who are not so good at. I have been a lot of a lot of people who have been a little bit of a lot of a lot of people who are going to be a little bit more than I have been able to do. I have been a lot of a lot of people who are a little bit more than I have been able to do it. I have been a lot of a lot of people who are not so much more than I have been a little bit more than I have been able to be a little bit more than I have been able to do. I have been a lot of a lot of a lot of people who are very much more than I have been able to do. I have been a lot of a lot of people who are a little bit more than I have been able to do. I have been a lot of a lot of the time and I have been able to do it. I have been a lot of a lot of people\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "# print(len(outputs[0].outputs[0].token_ids))\n",
    "print(outputs[0].outputs[0].text)\n",
    "# print(outputs[0].outputs[0].token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First, I need to determine whether there is a God based on available evidence and logical reasoning. I should start by understanding what \"God\" means in this context. \n",
      "\n",
      "First, definitions: \n",
      "\n",
      "1. **Theism**: Belief in the existence of a supreme being or gods.\n",
      "2. **Atheism**: Disbelief in the existence of a supreme being or gods.\n",
      "3. **Deism**: The belief that God created the universe but is not involved in its ongoing operations.\n",
      "4. **Pantheism**: The belief that God and the universe are one.\n",
      "5. **Polytheism**: The belief in multiple gods.\n",
      "\n",
      "Looking at evidence:\n",
      "\n",
      "- **Cosmological Argument**: The universe had a beginning, suggesting an intelligent cause.\n",
      "- **Teleological Argument**: Order and complexity suggest design.\n",
      "- **Moral Argument**: Moral principles suggest a divine lawgiver.\n",
      "- **Aesthetic Argument**: Beauty and order in nature suggest a designer.\n",
      "- **Existential Argument**: Humans seek meaning, suggesting a creator.\n",
      "- **Scientific Evidence**: Evolution and complexity point to a divine creator.\n",
      "\n",
      "Logical reasoning:\n",
      "\n",
      "- If God created the universe, He must have reasons.\n",
      "- If there are moral laws, a higher power must exist.\n",
      "- Rational beings would reason towards a first cause.\n",
      "\n",
      "But skepticism:\n",
      "\n",
      "- No empirical evidence.\n",
      "- Philosophical arguments can be debated.\n",
      "- Evolution challenges the need for a designer.\n",
      "\n",
      "Conclusion: \n",
      "\n",
      "Despite evidence and arguments, the question remains open. The answer depends on personal belief and understanding.\n",
      "</think>\n",
      "\n",
      "<answer>$\\boxed{Yes, based on evidence and logical reasoning, there is a God.}$<answer>\n",
      "</assistant>\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "# print(len(outputs[0].outputs[0].token_ids))\n",
    "print(outputs[0].outputs[0].text)\n",
    "# print(outputs[0].outputs[0].token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First, let me understand the question. The user is asking who decides what is right and wrong. I need to figure out the answer by breaking down the question.\n",
      "\n",
      "First, I should consider what it means to \"decide\" what is right and wrong. This must be a concept that has been established over time. I remember that moral philosophy deals with these concepts. \n",
      "\n",
      "One key figure here is Immanuel Kant. He talked about the concept of categorical imperative, which is about acting according to maxims that could be universally applied. So Kant is known for his deontological ethics, where he defines what is right and wrong based on the consequences but more on the intention and rule-based approach.\n",
      "\n",
      "Then there's John Stuart Mill, who introduced the principle of utilitarianism. He believed in the greatest happiness for the greatest number. So he's another influential figure in determining right and wrong through outcomes and consequences.\n",
      "\n",
      "But the question is asking \"who\" decides. So it's not just one person but a community or society. Societies, through their laws and norms, decide what is right and wrong based on their cultural, religious, or legal frameworks.\n",
      "\n",
      "I should also consider historical figures. In the Enlightenment, philosophers like Hobbes, Locke, and Rousseau influenced the idea of social contract theory, where the state's laws define right and wrong. So the answer is a combination of historical figures and social structures.\n",
      "\n",
      "Putting it all together, the user is likely looking for the concept that underlies these decisions. Kant's categorical imperative is a key part, as is Mill's utilitarianism. But the broader answer would be that it's a combination of individual moral theories and societal norms.\n",
      "</think>\n",
      "\n",
      "<answer>$\\boxed{Kant and Mill, along with a combination of historical moral theories and societal norms, determine what is right and wrong.}$<answer>\n",
      "</assistant>\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "# print(len(outputs[0].outputs[0].token_ids))\n",
    "print(outputs[0].outputs[0].text)\n",
    "# print(outputs[0].outputs[0].token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404\n",
      "Alright, so the user is asking, \"Where are all the aliens located?\" Hmm, that's a pretty bold and speculative question. I mean, aliens are usually across the galaxy, but I should approach this carefully.\n",
      "\n",
      "First, I need to consider the context. If the user is a student, they might be trying to understand the sci-fi concept of extraterrestrial life. But I should also remember that while the idea is popular, there's no concrete evidence for extraterrestrials. So, I shouldn't assume anything beyond what's widely accepted.\n",
      "\n",
      "I should provide a balanced answer. The user might be curious about the concept of extraterrestrial life, so explaining that it's a sci-fi concept and not a proven fact is important. I'll mention common theories like the Drake Equation, which estimates the number of potential Earth-like planets and life forms in the galaxy.\n",
      "\n",
      "Also, I should touch on the idea of biological contact, which some theories suggest might help us understand extraterrestrial life. But I should point out that this is speculative and not grounded in science.\n",
      "\n",
      "I should make sure to keep the language clear and informative, but also cautious about not giving away too much about the origins of life or other theories.\n",
      "\n",
      "So, I'll structure the answer by first acknowledging the speculative nature of the question, then explaining the concept of extraterrestrial life, and finally discussing theories related to biological contact without overstepping into areas where science doesn't provide clear answers.\n",
      "</think>\n",
      "\n",
      "The question about where all the aliens are located is a popular sci-fi concept, often explored through theories such as extraterrestrial life, the Drake Equation, or theories of biological contact. However, there is no concrete evidence to support the existence of extraterrestrial life or the idea of all aliens being located at specific locations. The concept is purely speculative and not based on any scientific evidence. If you're looking for more information on extraterrestrial life, I recommend exploring popular science fiction themes or theories like the Drake Equation.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "print(len(outputs[0].outputs[0].token_ids))\n",
    "print(outputs[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def format_check(completions):\n",
    "    pattern = r\"\\s*<think>.*?</think>\\s*<answer>.*?</answer>\\s*\"\n",
    "    matches = []\n",
    "    for content in completions:\n",
    "        start_tag = \"<think>\"\n",
    "        end_tag = \"</answer>\"\n",
    "\n",
    "        start_idx = content.find(start_tag)\n",
    "        end_idx = content.find(end_tag)\n",
    "\n",
    "        # make sure there is only one pair of <think> and <answer> tags\n",
    "        think_count = content.count(start_tag)\n",
    "        answer_count = content.count(end_tag)\n",
    "\n",
    "        if start_idx != -1 and end_idx != -1 and end_idx > start_idx and think_count == 1 and answer_count == 1:\n",
    "            content = content[start_idx:end_idx + len(end_tag)]\n",
    "\n",
    "        match = re.fullmatch(pattern, content, re.DOTALL)\n",
    "        matches.append(match)\n",
    "\n",
    "    return np.array([1.0 if m else 0.0 for m in matches])\n",
    "\n",
    "\n",
    "\n",
    "completion = '<think>\\nFirst, I need to simplify the expression \\\\(90r - 44r\\\\).\\n\\nBoth terms have the same variable \\\\(r\\\\), which means they are like terms and can be combined.\\n\\nI will subtract the coefficients: \\\\(90 - 44 = 46\\\\).\\n\\nTherefore, the simplified expression is \\\\(46r\\\\).\\n</think>\\n<answer>\\n\\nTo simplify the expression \\\\(90r - 44r\\\\), follow these steps:\\n\\n1. **Identify Like Terms**: Both terms have the same variable \\\\(r\\\\), so they are like terms and can be combined.\\n\\n2. **Subtract the Coefficients**:\\n   \\\\[\\n   90r - 44r = (90 - 44)r\\n   \\\\]\\n\\n3. **Calculate the Coefficient**:\\n   \\\\[\\n   90 - 44 = 46\\n   \\\\]\\n\\n4. **Write the Simplified Expression**:\\n   \\\\[\\n   46r\\n   \\\\]\\n\\n**Final Answer:**\\n\\\\[\\n\\\\boxed{46r}\\n\\\\]\\n</answer>'\n",
    "completions = [completion]\n",
    "format_scores = format_check(completions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xr1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
